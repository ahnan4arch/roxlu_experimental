* Log of streaming info
**  Tools:
- gstreamer
- vlc
- ffmpeg
- rtmpdump
- crtmpserver

** OUTCOME OF RESEARCH
- For testing use C++ RTMP Server
- Proof of concept code / crtmpserver settings / html: https://gist.github.com/410d53ca1b89283659ed
- Testing from command line with avconv (from libav): ./avconv -re -i f.mp4 -acodec copy -vcodec copy  -f flv -metadata streamName=test2 "tcp://127.0.0.1:6666"
- Testing avconv, when compiled with libmp3lame enabled (see configure), with sound works too: 
  ./avconv -re -i f.mp4 -b 500000 -s 320x180 -acodec libmp3lame -ab 96000 -ar 44100 -ac 2 -strict experimental -vcodec copy -g 25 -me_method zero -f flv -metadata streamName=video_test "tcp://127.0.0.1:6666" 
- Working stream to influxis:
	./avconv -re -i c.avi -b 500000 -s 320x180 -acodec libmp3lame -ab 96000 -ar 44100 -ac 2 -strict experimental -vcodec libx264 -g 25 -me_method zero -f flv -metadata streamName=livefeed "rtmp://gethinlewis.rtmphost.com/event/_definst_"
- @BBB (Dutch guy,  from #libav/#libav-devel helped a lot! Also @__tim
- For generated images/visuals, use the "flashsv", (Flash ScreenVideo) codec. ( http://www.mplayerhq.hu/DOCS/HTML/en/menc-feat-enc-libavcodec.html )
- For low latency you need an encoder with a low frame_size (AVCodecContext frame_size). i.e. Speex
- Speex is implemented in FLV with 1 channel, 16Khz. ( http://en.wikipedia.org/wiki/Speex )
- Mac hardware accel: http://developer.apple.com/library/mac/#technotes/tn2267/_index.html 
- Nellymoser audio codec, optimized for streaming: ./avconv -re -i c.avi -b 100000 -s 312x176 -acodec nellymoser -ab 32000 -ar 16000 -ac 1 -strict experimental -vcodec libx264 -g 25 -me_method zero -f flv -metadata streamName=video_test "tcp://127.0.0.1:6666"
- Convert video to a smaller size: ./avconv -i test.mp4 -acodec copy -vf "scale=320:180" -vcodec libx264  small_video.mp4

** GSTREAMER
- appsrc
- gst-launch -v -e videotestsrc ! ffenc_flv ! flvmux ! filesink location="test.flv"
- gst-launch -v videotestsrc ! theoraenc ! oggmux ! tcpserversink host=192.168.1.35 port=8080

- http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-bad-plugins/html/gst-plugins-bad-plugins-rtmpsink.html
- gst-inspect rtmpsink
- run with lots of output: ./appsrctest --gst_debug-level=5 --gst-debug=*:8 &>err.log   
- show message:s: ./appsrctest --gst_debug-level=5 --gst-debug=*:3
- plugins are in the "ext" subdir
- good article on gstreamer (pads, how the pipeline syntax works, queue ! etc..): http://www.cin.ufpe.br/~cinlug/wiki/index.php/Introducing_GStreamer

** COMPILING
-- build: the machine you are building on
-- host: the machine you are building for
-- target: the machine that gcc will product code for
-- when you only want to compile one plugin (i.e. png from the good plugins) first make sure to configure with the 
correct settings (ldflags, cflags, etc..), then go into "ext/plugin-name" directory and type: make && make install

** Code
- Example of streaming RGB data into gstreamer: https://gist.github.com/725122/16ceee88aafae389bab207818e0661328921e1ab
- RGB instream, goom http://gstreamer.freedesktop.org/data/doc/gstreamer/head/gst-plugins-good-plugins/html/gst-plugins-good-plugins-goom.html
- Stack overflow question: http://stackoverflow.com/questions/5314562/pushing-images-into-a-gstreamer-pipeline 
- Gstreamer Formats: http://gstreamer.freedesktop.org/data/doc/gstreamer/head/pwg/html/section-types-definitions.html
- Streaming to web: http://bitsearch.blogspot.nl/2011/03/streaming-to-html5-web-page.html
- Ogg streaming to web: http://planet.xiph.org/ 
- Writing appsrc data to video file: http://gstreamer-devel.966125.n4.nabble.com/appsrc-to-filesink-td970865.html 
- Example I wrote which creates an appsrc and uses the pull model to send video data: https://gist.github.com/56672069fcf6057c7492
- Example with app settings, blocking, timestamp: http://users.design.ucla.edu/~acolubri/test/gstreamer/appsrc_filesink.c
- Good info on the glib mainloop and how it works for the push model of an app src: http://amarghosh.blogspot.nl/
- GStreamer based live-stream: http://landell.holoscopio.com/
- Mac hardware encoding: http://developer.apple.com/library/mac/#technotes/tn2267/_index.html
- MPEG TS over TCP to rtmpserver https://lists.libav.org/pipermail/libav-api/2011-October/000219.html 
- gstreamer -> c++ rtmp server, something wrong with rtmp sink: https://groups.google.com/forum/?fromgroups#!topic/c-rtmp-server/f1dmcMyjpro
- swscale-example: http://svn.tribler.org/ffmpeg/trunk/libswscale/swscale-example.c

** FFMPEG:
- How to live stream to (good article): http://wiki.rtmpd.com/tutorial_live_stream_file
- Working command which streams a file to crtmpserver (though still hangs a bit)
	ffmpeg -i f.mp4 -re -vcodec libx264 -vpre default -vpre baseline -b 500000 -g 25 -me_method zero -acodec aac -ab 9600 -ar 48000 -ac 2 -vbsf h264_mp4toannexb -f mpegts tcp://127.0.0.1:9999
- Streaming segmented video to web: http://www.ioncannon.net/programming/452/iphone-http-streaming-with-ffmpeg-and-an-open-source-segmenter/
** LibAV
- Using avconv to create a rtmp stream from mp4 to crtmpserver: ./avconv -re -i f.mp4 -f flv -metadata streamName="test" tcp://localhost:6666
- Playback using: https://github.com/revmischa/simplestream 
- Flash livestream playback is different then standard playback
- Nice article how libav is structured and how to use it (mpegts): http://plagatux.es/2011/07/using-libav-library/ 
- Doxygen libav: http://libav.org/doxygen/master/structAVIOContext.html
- LibAV format conversion with filter: https://gist.github.com/1834778
- Post on RGB->YUV -> RTSP http://libav-users.943685.n4.nabble.com/av-write-frame-not-returning-td3760977.html
- Nice post on libav (using avi as input), example of swscale: http://blog.tomaka17.com/2012/03/libavcodeclibavformat-tutorial/
- Has converted RGB-> YUV http://www.afiestas.org/:
- AFiestas: https://twitter.com/afiestas, on KDE freenode
- Swscale with AVFrame: http://libav-users.943685.n4.nabble.com/sws-scale-crashes-td2309626.html
- VBV buffer (MPEG) http://www.bretl.com/mpeghtml/VBV.HTM
- To check if a particular video ENCODER is supported, use the "bin/avconv -codec" list.
- Information about PTS (presentation timestamp): http://libav-users.943685.n4.nabble.com/libx264-xxx-non-strictly-monotonic-PTS-td3275701.html#a3297282
- Transcoding images to video: https://github.com/krieger-od/imgs2video
- Info on PTS and DTS: http://dranger.com/ffmpeg/tutorial05.html
- Setting h264 specs: http://libav-users.943685.n4.nabble.com/coded-frame-gt-pts-not-being-set-by-libx264-after-call-to-avcodec-encode-video-td946931.html
- IRC log of Ruggles (audio dude of libav): https://gist.github.com/599c487e9c3362ae1710 
- Some info an libav / h264 encoding settings: http://www.mplayerhq.hu/DOCS/HTML/en/menc-feat-enc-libavcodec.html 
- Good tutorial on muxers / how structures work in libav: http://libav.org/doxygen/master/group__lavf__decoding.html
*** LIBAV avconv parameters
    - RTML:
      - rtmp_buffer: Set client buffer time, default is 3000 millis
      - rtmp_live: Set this for live streaming (no seek possible)

** AVCONV
*** Settings
**** Audio
     - ar: Audio Sampling rate in HZ (44100) (FLV supports i.e. 44100, 22050
     - ac: Number of channels
**** Video
     - avconv options for h264: https://gist.github.com/39ad734538cb33920001 (see ./avconv --help)
     - example of somewhat good settings https://gist.github.com/c990997ca26291a5cdb5
     - re: Read input at native frame rate
     - crf: h264 constant quality (VBR) setting.  (i.e. use 20 for a good result)
     - g: group of picture size. YouTube recommends 2 seconds. 
     - for AVConv h264 related settings (such as b frames) see the "5.4 Codec AVOptions" section of avconv   
     - r: output framerate (i.e. 60) 
**** Examples:
***** Piping audio + video at 60 FPS
      This example shows that we're expecting 60 FPS as input, but due to thread locking or other
      things which need the CPU, we might give less then 60 frames as second. Therefore we use a
      output framerate of 30. This seems to fix sync issues with audio and video.

      <code>
     ${d}/build/bin/avconv \
    -f rawvideo \
    -pix_fmt rgb24 \
    -r 60 \
    -s 320x240 \
    -i ${video_pipe} \
    -ac 2 \
    -ar 44100 \
    -f f32le \
    -i ${audio_pipe} \
    -loglevel debug \
    -acodec libmp3lame \
    -vcodec libx264 \
    -preset ultrafast \
    -tune zerolatency \
    -bf 2 \
    -threads 4 \
    -g 60 \
    -f flv \
    -r 30 \
    -metadata streamName="test_video" \
    tcp://localhost:6666 
    <code>

** H264
- Good introduction to profiles: http://www.iqeye.com/iqeye/H.264_Considerations.pdf
- IRC: #shariman0 has done some things with pipeing video/audio
- IRC: #BBB-work, dutch guy, works at google
- IRC: killian(?) belgian guy, knows a lot about encoding
*** Settings:
    - Youtube / google recommended settings: http://support.google.com/youtube/bin/static.py?hl=en&page=guide.cs&guide=1728585&topic=1731151&answer=1723080
    - Profiles:
      There are 8 profiles (see above link): high stereo,  Hi444pp, hi10p, hip, xp, mp, bb, cbp
      A profile defines the maximum number of possible features, see this page for what features
      there are possible for a profile: http://www.iqeye.com/iqeye/H.264_Considerations_III.pdf
    - Presets: http://dev.gentoo.org/~beandog/x264_preset_reference.html
    - CBR / VBR Bitrate: 
      you can use constant bit rate (CBR) or variable bit rate (VBR). CBR is used to 
      make sure you don't exceed a certain bandwidth. When CBR used the image quality will be 
      less to make sure the desired bandwidth is not exceeded.
      VBR: use this when image quality is important (i.e. surveillance video)
    - You can use the "-crf" to set quality based VBR. Value (0-51). 51 is worst quality
    - To maximize quality use VBR and Main (or higher) profile
    - Good article on h264 rate control settings: http://www.pixeltools.com/rate_control_paper.html
    - QP: Quantization Parameter: how much spatial detail is saved. When QP is small almost all detail is save.
      When QP is high, quality is lost and less data is needed to encode
    - GOP: Group of Pictures (GOP). The Group of Picture concept is inherited from MPEG and refers to an I-picture, 
      followed by all the P and B pictures until the next I picture. 
      
** VIDEO CODECS 
- You have I, B, and P frames.
	- I = intra frame = full image
	- P = predicted frame = like an delta/diff, looks back for info.
	- B = bidirectional frame = sames as P, but look back and forward for info!
- To view frames correctly there are PTS and DTS. 
	- PTS: presentation time stamp
	- DTS: decoding timestamp
- Frames can be stored in a different order then that they should be played, 
  i.e. When we have: I B B P, this is stored as I P B B.  The B B need the 
  information from P, so we decode this first, but B is shown before P. 

** CRTMP SERVER
- Documentation: http://wiki.rtmpd.com/documentation
- Not sure if it's avconv, but when transcoding using libmp3lame sound works! (/avconv -re -i f.mp4 -b 500000 -s 320x180 -acodec libmp3lame -ab 96000 -ar 44100 -ac 2 -strict experimental -vcodec copy -g 25 -me_method zero -f flv -metadata streamName=video_test "tcp://127.0.0.1:6666")

** LIBAV CODING INFO
- av_register_all() must be called before you can use "any" of av.
- av_register_all() also initializes the codecs
- Getting this error in crtmpserver: /inboundliveflvprotocol.cpp:102 Frame too large: 16246266 
- when you create a stream with a specific codec id, you don't have to allocate the codeccontext. ( http://libav.org/doxygen/master/group__lavf__core.html#ga5747bd011dd73be7963a7d24da96f77c )
- A muxer only sees streams
- For a audio codec context you must set the format member (sample_fmt)
- AVPacket - contains compressed data
- AVFrame - contains uncompressed data

	LibAV: dts and pts
	------------------
	dts: decoding timestamp
	pts: presentation timestamp

	Because video encoding using the concepts of B, I and P frames which are interleaved with audio
	it can happen that frame 3 is decoded before frame 1. Therefore there is a separate timestamp
	for decoding and presentation in the codec. These timestamps are related to the codec (<--- is this correct).

	Then the muxer/container has a pts/dts for both audio/video. 

	* AVFrame.pts is in units of codec->time_base
	* AVFrame.pts must be set by the user when encoding. 
	* AVPacket.pts/dts  is in units of stream->time_base
	

	-- from: ( http://libav-users.943685.n4.nabble.com/PTS-and-DTS-in-a-packet-aren-t-in-timescale-units-td943755.html#a943760 )
		There's: 
			- p_format_ctx_->streams[video_stream_]->codec->time_base 
			- p_format_ctx_->streams[video_stream_]->time_base 
		The latter is what I use to convert my PTS values, and it seems to work fine. 

	--  pkt->pts can be AV_NOPTS_VALUE if the video format has B frames, so it is better to rely on pkt->dts if you do not decompress the payload.
		(from: http://dranger.com/ffmpeg/functions.html#av_read_frame )

	-- when you are encoding, you set pts on the AVFrame you send to  the encoder
	the encoder will properly set pts and dts on the AVPacket it gives you back.

	-- the time_base is a rational number (numerator/denumerator). The timestamp uses this
	to calculate the time to play back the audio/video. When the time_base is 1/25, aka
	25 frames per second, the pts/dts increment like: 1,2,3,4,5,6,7 etc.. the "real values"
	are then: 1*(1/25), 2*(1/25), 3*(1/25) ... 7*(1/25) etc..

	- @elenril: the muxer timebase is chosen by the muxer
	- @elenril: you need to rescale AVPacket values from encoder timebase to the muxer timebase manually
	- @elenril: but strictly speaking the API requires you to set the timebase yourself before opening the encoder
	- @elenril: i suppose you'll use some microsecond or nanosecond clock, so just set your timebase to microseconds or nanoseconds, i.e. 1/1E6 or 1/1E9

- AVOutputFormat: is just a "hint", it's a static variable
- When debugging you can use tools like this to inspect the bytes:
	- wireshark
	- flv meta: http://code.google.com/p/flvmeta/
	- avprope (show packets), which is part of libav
- Set level with: av_log_set_level (see libavutil/log.h)
- Set the correct stream index of a AVPacket after you've encoded the frame and passing it to interleave_write
- Check if an output format has global headers -> set codec flags if needed.
- H264 variable bitrate: http://stackoverflow.com/questions/10016761/encapsulating-h-264-streams-variable-framerate-in-mpeg2-transport-stream 
- Use this to get the bytes a audio buffer needs to encode data: 
	- int out_size = av_samples_get_buffer_size(&out_line, 1, c->frame_size, AV_SAMPLE_FMT_S16, 0); // we can use this to get the total size
	
** HTML 5 streaming standard:
- Standard for web streaming:  http://tools.ietf.org/html/draft-pantos-http-live-streaming-07
- http://tools.ietf.org/html/draft-pantos-http-live-streaming-01

** RAW ENCODING:
- flv standard: http://download.macromedia.com/f4v/video_file_format_spec_v10_1.pdf
- x264 info: http://en.wikipedia.org/wiki/H.264/AVC
- x264 NAL units: http://en.wikipedia.org/wiki/Network_Abstraction_Layer#NAL_Units_in_Byte-Stream_Format_Use
- x264 options: http://mewiki.project357.com/wiki/X264_Settings
- gob size: This option sets the minimum and maximum number of frames before a key frame has to be inserted by x264.

** RTMPDUMP
 	- 	replace -soname with -dylib_install_name (in ./rtmpdump/librtmp/Makefile)
 		Make sure that there are no spaces between the commas:
 		SO_LDFLAGS_posix=-shared -Wl,-dylib_install_name,$@

 	- 	Add this on top of BOTH makefiles:
		DESTDIR=/Users/diederickhuijbers/c++/__PLAYGROUND__/libav/build/

	- 	Compile with (on mac)
		make sys=darwin
** NAMED PIPES
- act as FIFO
- by default read/write blocks
- can only be opened as read only or write only
- Good article on names pipes: http://developers.sun.com/solaris/articles/named_pipes.html
- Article on pipes with code: http://www.cs.uml.edu/~fredm/courses/91.308/files/pipes.html
- avconv tries to open the first pipe you pass into it before it opens the second one. Because a pipe will block
you need to make sure that you only open the pipe when you have enough data for it that will be read per "frame"
by avconv (or i.e. ffmpeg). When you try to open two pipes directly it will fail because avconv waits for data
directly when the first pipe is opened.
** JUST SOME CODING SNIPPETS
   Read a file into a std::vector, use std::copy() and std::istream_iterator/back_inserter
   When you print a uint as:

   uint32_t v = 0x04030201; 
   uint8_t* p = &v;
   printf("%02X %02X %02X %02X", *(p+0), *(p+1), *(p+2), *(p+3))

   you get: 01 02 03 04

   This is because on a little endian machine the least significant bit (the one which determines is a 
   value is odd or even (the 01 value above), is stored on the lowest memory address.  So when you i.e.
   have a buffer of uint8_t and you print this 4 bytes which show: 01 02 03 04 and you want to read them
   as LE, you need to know that the 01 should be stored at the lowest memory address. But be aware,
   if you want to extract the lowest bit, you would do: v & 0x000000FF.  
** KRADRADIO
*** Architecture
    - Inspired by XMMS2
    - KRAD is a daemon
    - Compositor (video) / audio (mixer)
    - Tools directory: tiny libs
    - Audio input: JACK (audio mixer) or pulse audio
    - Video input: Video4Linux, DecklinkLinke 
    - daemon just runs with a name
    - the client sends data
    - icecast for streaming
    - use webm/vp8 or ogg/theora
    - http progressive download is the way to go for live streaming
    - webrtc is the thing to go
** WEBM
- Uses Matroska file-format wth ebml (binary xml)
- Matroska needs at least these elements to be playable (http://matroska.org/technical/order/index.html)
  - Segment Info
  - Track Info
  - Clusters
- Segment Info + Track info must be stored before Clusters
- Each TrackEntry must have a CodecID defined
- Codec uses encoded data in the stream, but it can also contain initialisation information (i.e. h264 headers)
- For live streaming the segment must use an UNKOWN SIZE: (http://matroska.org/technical/streaming/index.html)
- EBML syntax: [ID, SIZE, DATA]. The DATA can be other EBML tags
- EBML elements can have an unknown size. When the size is unknown
  the contained data must be a EBML-element-list (nesting). The end of the element list
  is determined by the ID of the element. When an element that isn't a sub-element
  of the element with unknown size (the parent), the element list is ended.
  In normal language: a parent can have an undetermined number of children. You know
  when the end of the children have been reached when the ID of the processed child
  is not the same as the "first" one. So: <parent><a /> <a /> <a /> <b /> </parent>
- When you store EBML elements like: <div><p /> <p /> <p /> </div>, you cant set a size for the div which 
  has the number of bytes for all <p> elements. (in EBML.cpp, you set EBML_SIZE_MODE_DONT_SET for the <p> elements...)
  .. I need to recode ebml a bit; it's not really clean.. as the sizes are only calculated when you flush().
- Utility to check if your data is correct: MKVInfo http://www.bunkus.org/videotools/mkvtoolnix/doc/mkvinfo.html
- MKVInfo download: http://www.bunkus.org/videotools/mkvtoolnix/macos/ (see package contents for all tools) 
  The mkvinfo util is extremelly helpfull!
- Great explanation about the Matroska + EBML file format: http://matroska.org/files/matroska.pdf
- Some documentation: http://www.webmproject.org/tools/encoder-parameters/
- _GENERATING WEBM_ Check vpxenc.c from the libvpx library, this shows how to mux webm.
- When you encode RGB24 with libvpx you need to convert the RGB data to YUV, I420 first. You can use libyuv for this.
- The examples of webm show how to create a "ivf" file. With MKVToolnix you can convert this to a webm file.
- You can use the utils vpxenc/vpxdec to encode/decode webm to and from ivf files. 
-  ./vpxenc --i420 -w 960 -h 540 -o output.webm output.ivf 
- ./vpxdec --i420 --progress --limit=100 -o output.ivf web.webm  
- WEBM matroska format is build with these parts:
  - header info 
  - segment info
  - tracks
- the h264 library has a nice clean ebml implementation
- For webm, and constant framerate you use these values for the timescale numerator and denominator 
  settings: I.e for 25 fps, you use: 
  numerator = 25
  denominator = 1

  Then when you encode and you need to calculate the current timestamp value, you use:
  time_per_frame = fps_denominator / fps_numerator * current_frame_number

  ++++++++++++++++++++++++++++++++++++++++++++++++++++
  
   _IMPORTANT_:
   When your framerate is not stable (i.e. webcam), you need to make sure to adjust the 
   fps_denominator and fps_numerator values when you calculate the current timestamp.

   ++++++++++++++++++++++++++++++++++++++++++++++++++

- Article on adaptive http streaming: http://blog.gingertech.net/2010/10/09/adaptive-http-streaming-for-open-codecs/
*** VPX encoding
**** General:
    - vpx is a general term; you can i.e. have vp8,vp9,vp10 
    - Samples: http://www.webmproject.org/tools/vp8-sdk/samples.html
    - Simple encoder: http://www.webmproject.org/tools/vp8-sdk/example__simple__encoder.html 
    - See vp8/common/onyx.h for some mode "settings". Search for streaming.
    - See tools/krad_vpx/krad_vpx.c for setup of vpx in krad_radio
    - IRC: #vp8, #libav, #xiph
    - See libyuv for yuv conversion
    - WebRTC, related in browser real time media http://www.webrtc.org/
**** Parameters:
     - quality parameters: 
        -- best (Too slow very good quality)
        -- good : used by most people. With "good" there are 6 other settings:
     - cpu-used : 0 - 5
        -- rt : real-time 
     - rate control (VBR, CBR, CQ):
        -- end-usage: vbr, cbr, cq  (cq constrained quality, form of variable bitrate)
        -- cg-level: ranges 0-63, default 10
        -- min-q, max-q: when 'cq' is used, these are the lower and upper limits between 
           the encoder will stay
    - passes: 1 or 2, for streaming 1
    - timebase
**** API
    - vpx_codec_enc_cfg_t
    - vpx_codec_pkt_t
    - vpx_codec_enc_config_default
    - vpx_codec_enc_init
    - vpx_codec_control
    - vpx_usec_timer_start
    - vpx_codec_encode
    - vpx_usec_timer_mark
*** Parts to implement for streaming
    - Test files: http://www.matroska.org/downloads/test_w1.html (also for streaming)
**** Segment Information:
     - related to whole file
     - title 
     - unique id
**** Track Info
     - has basic information about each of the tracks
     - what resolution
     - what codec 
     - codec private data
**** Clusters
     - contains all the video frames and audio for each track
       


    
     
** MP3 encoding with liblame
   - example: http://stackoverflow.com/questions/2495420/is-there-any-lame-c-wraper-simplifier-working-on-linux-mac-and-win-from-pure/2496831#2496831
   - see DarkIce source code, the file BufferedSink has a nice example of how to create a ring buffer
   - Very good article on building a radio station: http://files.dyne.org/muse/opensource-radio-streaming.pdf
